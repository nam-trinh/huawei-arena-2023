{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers accelerate bitsandbytes scipy ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd11b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psutil ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9835d009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.33.2\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from typing import List\n",
    "import sqlite3\n",
    "from tabulate import tabulate\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "from torch.multiprocessing import Pool, Process, set_start_method\n",
    "import ray\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce4269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f98a21d",
   "metadata": {},
   "source": [
    "# Load the model\n",
    "\n",
    "Set parameter load_in_4bit=True for a model size of 12GB, otherwise the model will be too big. \n",
    "If you work on A100 machine, parameter torch_dtype=torch.bfloat16 can be set to True instead.\n",
    "\n",
    "Can read more here: https://huggingface.co/blog/4bit-transformers-bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0da6922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdb1a85c12248e792fec22ee5e7c2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPTBigCodeForCausalLM(\n",
       "  (transformer): GPTBigCodeModel(\n",
       "    (wte): Embedding(49152, 2816)\n",
       "    (wpe): Embedding(8192, 2816)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-35): 36 x GPTBigCodeBlock(\n",
       "        (ln_1): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTBigCodeAttention(\n",
       "          (c_attn): Linear(in_features=2816, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=2816, out_features=2816, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTBigCodeMLP(\n",
       "          (c_fc): Linear(in_features=2816, out_features=11264, bias=True)\n",
       "          (c_proj): Linear(in_features=11264, out_features=2816, bias=True)\n",
       "          (act): PytorchGELUTanh()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2816, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sql_model_name = \"defog/sqlcoder\"\n",
    "sql_model_name = \"seeklhy/codes-3b\"\n",
    "device = torch.device('cpu')\n",
    "sql_tokenizer = AutoTokenizer.from_pretrained(sql_model_name)\n",
    "sql_model = AutoModelForCausalLM.from_pretrained(\n",
    "    sql_model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # load_in_4bit=True,\n",
    "    # device_map=device,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "answer_model = sql_model\n",
    "answer_tokenizer = sql_tokenizer\n",
    "\n",
    "# answer_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# token = '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'\n",
    "\n",
    "# answer_tokenizer = AutoTokenizer.from_pretrained(answer_model_name, token=token, cache_dir='/home/mpham/workspace/huawei-arena-2023/.cache')\n",
    "# answer_model = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model='openai-gpt',\n",
    "#     # torch_dtype=torch.bfloat16,\n",
    "#     # device_map=\"auto\",\n",
    "#     # token=token\n",
    "# )\n",
    "sql_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1765f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTBigCodeForCausalLM(\n",
       "  (transformer): GPTBigCodeModel(\n",
       "    (wte): Embedding(49152, 2816)\n",
       "    (wpe): Embedding(8192, 2816)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-35): 36 x GPTBigCodeBlock(\n",
       "        (ln_1): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTBigCodeAttentionLayerBetterTransformer(\n",
       "          (c_attn): Linear(in_features=2816, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=2816, out_features=2816, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTBigCodeMLP(\n",
       "          (c_fc): Linear(in_features=2816, out_features=11264, bias=True)\n",
       "          (c_proj): Linear(in_features=11264, out_features=2816, bias=True)\n",
       "          (act): PytorchGELUTanh()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2816, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optimum\n",
    "sql_model.to_bettertransformer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e668ed70-4cc3-43c4-99b3-9710823201f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_query(model, tokenizer, question: str, db_schema: str, tables_hints: List[str]) -> str:\n",
    "    # Implement logic to generate an SQL query based on the question and table hints.\n",
    "    # Replace the \"pass\" with a calling function to LLM\n",
    "    \n",
    "    # Handle the case when table hints are empty or invalid.\n",
    "    if not tables_hints:\n",
    "        # Default behavior: Query all tables\n",
    "        pass\n",
    "    \n",
    "    # Handle the general case\n",
    "    # Example: \"SELECT COUNT(*) FROM customers\"\n",
    "    prompt = generate_sql_query_generation_prompt(question, db_schema, tables_hints)\n",
    "    eos_token_id = tokenizer.convert_tokens_to_ids([\"```\"])[0]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\") # or to(\"cpu\")\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=eos_token_id,\n",
    "        pad_token_id=eos_token_id,\n",
    "        max_new_tokens=400,\n",
    "        do_sample=False,\n",
    "        num_beams=1\n",
    "    )\n",
    "    \n",
    "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    sql_query = outputs[0].split(\"```sql\")[-1].split(\"```\")[0].split(\";\")[0].strip() + \";\"\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6747ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_db_schema = \"\"\"\n",
    "CREATE TABLE products (\n",
    "  product_id INTEGER PRIMARY KEY, -- Unique ID for each product\n",
    "  name VARCHAR(50), -- Name of the product\n",
    "  price DECIMAL(10,2), -- Price of each unit of the product\n",
    "  quantity INTEGER  -- Current quantity in stock\n",
    ");\n",
    "\n",
    "CREATE TABLE customers (\n",
    "   customer_id INTEGER PRIMARY KEY, -- Unique ID for each customer\n",
    "   name VARCHAR(50), -- Name of the customer\n",
    "   address VARCHAR(100) -- Mailing address of the customer\n",
    ");\n",
    "\n",
    "CREATE TABLE salespeople (\n",
    "  salesperson_id INTEGER PRIMARY KEY, -- Unique ID for each salesperson\n",
    "  name VARCHAR(50), -- Name of the salesperson\n",
    "  region VARCHAR(50) -- Geographic sales region\n",
    ");\n",
    "\n",
    "CREATE TABLE sales (\n",
    "  sale_id INTEGER PRIMARY KEY, -- Unique ID for each sale\n",
    "  product_id INTEGER, -- ID of product sold\n",
    "  customer_id INTEGER,  -- ID of customer who made purchase\n",
    "  salesperson_id INTEGER, -- ID of salesperson who made the sale\n",
    "  sale_date DATE, -- Date the sale occurred\n",
    "  quantity INTEGER -- Quantity of product sold\n",
    ");\n",
    "\n",
    "CREATE TABLE product_suppliers (\n",
    "  supplier_id INTEGER PRIMARY KEY, -- Unique ID for each supplier\n",
    "  product_id INTEGER, -- Product ID supplied\n",
    "  supply_price DECIMAL(10,2) -- Unit price charged by supplier\n",
    ");\n",
    "\n",
    "-- sales.product_id can be joined with products.product_id\n",
    "-- sales.customer_id can be joined with customers.customer_id\n",
    "-- sales.salesperson_id can be joined with salespeople.salesperson_id\n",
    "-- product_suppliers.product_id can be joined with products.product_id\n",
    "\"\"\"\n",
    "\n",
    "question = \"List 5 highest sales product name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8e25714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 14:39:56,184\tINFO worker.py:1476 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPUs: 20\n",
      "Ray started!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The remote function __main__.generate_query_parallel is too large (5870 MiB > FUNCTION_SIZE_ERROR_THRESHOLD=95 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:27\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/ray/remote_function.py:134\u001b[0m, in \u001b[0;36mRemoteFunction.__init__.<locals>._remote_proxy\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_remote_proxy\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remote\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/ray/_private/auto_init_hook.py:24\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     23\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py:310\u001b[0m, in \u001b[0;36m_tracing_task_invocation.<locals>._invocation_remote_span\u001b[0;34m(self, args, kwargs, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ray_trace_ctx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ray_trace_ctx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[1;32m    313\u001b[0m tracer \u001b[38;5;241m=\u001b[39m _opentelemetry\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mget_tracer(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/ray/remote_function.py:301\u001b[0m, in \u001b[0;36mRemoteFunction._remote\u001b[0;34m(self, args, kwargs, **task_options)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickled_function \u001b[38;5;241m=\u001b[39m pickle_dumps(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function,\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize the function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_descriptor\u001b[38;5;241m.\u001b[39mrepr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_export_session_and_job \u001b[38;5;241m=\u001b[39m worker\u001b[38;5;241m.\u001b[39mcurrent_session_and_job\n\u001b[0;32m--> 301\u001b[0m     \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_actor_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[1;32m    304\u001b[0m args \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m args\n",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/ray/_private/function_manager.py:249\u001b[0m, in \u001b[0;36mFunctionActorManager.export\u001b[0;34m(self, remote_function)\u001b[0m\n\u001b[1;32m    246\u001b[0m function \u001b[38;5;241m=\u001b[39m remote_function\u001b[38;5;241m.\u001b[39m_function\n\u001b[1;32m    247\u001b[0m pickled_function \u001b[38;5;241m=\u001b[39m remote_function\u001b[38;5;241m.\u001b[39m_pickled_function\n\u001b[0;32m--> 249\u001b[0m \u001b[43mcheck_oversized_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpickled_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mremote function\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m key \u001b[38;5;241m=\u001b[39m make_function_table_key(\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemoteFunction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker\u001b[38;5;241m.\u001b[39mcurrent_job_id,\n\u001b[1;32m    258\u001b[0m     remote_function\u001b[38;5;241m.\u001b[39m_function_descriptor\u001b[38;5;241m.\u001b[39mfunction_id\u001b[38;5;241m.\u001b[39mbinary(),\n\u001b[1;32m    259\u001b[0m )\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker\u001b[38;5;241m.\u001b[39mgcs_client\u001b[38;5;241m.\u001b[39minternal_kv_exists(key, KV_NAMESPACE_FUNCTION_TABLE):\n",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/ray/_private/utils.py:945\u001b[0m, in \u001b[0;36mcheck_oversized_function\u001b[0;34m(pickled, name, obj_type, worker)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     error \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is too large (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m MiB > FUNCTION_SIZE_ERROR_THRESHOLD=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m MiB). Check that its definition is not implicitly capturing a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m         ray_constants\u001b[38;5;241m.\u001b[39mFUNCTION_SIZE_ERROR_THRESHOLD \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m),\n\u001b[1;32m    944\u001b[0m     )\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error)\n",
      "\u001b[0;31mValueError\u001b[0m: The remote function __main__.generate_query_parallel is too large (5870 MiB > FUNCTION_SIZE_ERROR_THRESHOLD=95 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-23 14:40:52,754 E 3629285 3629285] (raylet) node_manager.cc:3007: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4ba428b7ea3628d48b1711c44a786bd5595268b162ee4757bb652f6c, IP: 136.206.48.20) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 136.206.48.20`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_cpus = psutil.cpu_count(logical=True)\n",
    "print('Number of available CPUs:', num_cpus)\n",
    "\n",
    "# Start Ray cluster\n",
    "ray.init(num_cpus=num_cpus, ignore_reinit_error=True)\n",
    "print('Ray started!')\n",
    "\n",
    "pipe_id = ray.put(generate_sql_query)\n",
    "\n",
    "# Define the fixed parameters (db_schema and tables_hints)\n",
    "db_schema = sales_db_schema  # Your fixed database schema\n",
    "tables_hints = None  # Your fixed table hints\n",
    "\n",
    "# Define the model and tokenizer\n",
    "model = sql_model  # Your model\n",
    "tokenizer = sql_tokenizer  # Your tokenizer\n",
    "\n",
    "# List of input questions (you can create a list of questions to process in parallel)\n",
    "question = \"List 5 highest sales product name\"\n",
    "\n",
    "# Define a remote function for parallel execution\n",
    "@ray.remote\n",
    "def generate_query_parallel(generate_sql_query, question):\n",
    "    return generate_sql_query(sql_model, sql_tokenizer, question, db_schema=db_schema, tables_hints=tables_hints)\n",
    "\n",
    "# Parallelize the execution using Ray\n",
    "results = ray.get(generate_query_parallel.remote(question))\n",
    "\n",
    "# Results will contain a list of generated SQL queries\n",
    "for sql_query in results:\n",
    "    print(sql_query)\n",
    "\n",
    "# Shut down Ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f0b70-536b-4a3f-bc78-e0d67a8a53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_context(model, tokenizer, question: str, schema: List[str]) -> str:\n",
    "    answer_generation_prompt = generate_answer_generation_prompt(question, schema)\n",
    "    print('answer prompt:', answer_generation_prompt)\n",
    "    \n",
    "    ### Use the same procedure\n",
    "    \n",
    "    eos_token_id = tokenizer.convert_tokens_to_ids([\"```\"])[0]\n",
    "    inputs = tokenizer(answer_generation_prompt, return_tensors=\"pt\").to(\"cuda\") # or to(\"cpu\")\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=eos_token_id,\n",
    "        pad_token_id=eos_token_id,\n",
    "        max_new_tokens=400,\n",
    "        do_sample=False,\n",
    "        num_beams=1\n",
    "    )\n",
    "\n",
    "    # answer = model(\n",
    "    #     answer_generation_prompt,\n",
    "    #     do_sample=False,\n",
    "    #     # top_k=10,\n",
    "    #     num_return_sequences=1,\n",
    "    #     eos_token_id=tokenizer.eos_token_id,\n",
    "    #     max_length=200,\n",
    "    # )\n",
    "\n",
    "    \n",
    "    # for seq in sequences:\n",
    "    #     print(f\"Result: {seq['generated_text']}\")\n",
    "    \n",
    "    \n",
    "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    answer = outputs[0].split(\"```ans\")[-1].split(\"```\")[0].split(\";\")[0].strip() + \";\"\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348fa39",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_QUERY_PROMPT_TEMPLATE = \"\"\"### Instructions:\n",
    "Your task is to convert a question into a SQL query, given a SQLlite database schema.\n",
    "Adhere to these rules:\n",
    "- **Deliberately go through the question and database schema word by word** to appropriately answer the question\n",
    "- **Use Table Aliases** to prevent ambiguity. For example, `SELECT table1.col1, table2.col1 FROM table1 JOIN table2 ON table1.id = table2.id`.\n",
    "- When creating a ratio, always cast the numerator as float\n",
    "### Input:\n",
    "Generate a SQL query that answers the question `{question}`.\n",
    "This query will run on a database whose schema is represented in this string:\n",
    "{db_schema}\n",
    "\n",
    "These are some hints to help you in this task. \n",
    "Some of these hints can be wrong, so only use relevant ones:\n",
    "{tables_hints}\n",
    "\n",
    "### Response:\n",
    "Based on your instructions, here is the SQL query I have generated to answer the question `{question}`:\n",
    "```sql\n",
    "\"\"\"\n",
    "\n",
    "ANSWER_GENERATION_PROMPT_TEMPLATE = \"\"\"\n",
    "Generate a suitable answer to a prompt based on the extracted tabular information.\n",
    "The information extracted from the database is as follows:\n",
    "{returned_schema}\n",
    "Each row represents a data point, and the columns are separated by \"|\".\n",
    "Your answer should be short, concise and straight to the point.\n",
    "The prompt is as followed: `{question}`\n",
    "\n",
    "### Response:\n",
    "Based on your instructions, here is the answer I have generated to give an appropriate response:\n",
    "```ans\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_query_generation_prompt(question, db_schema, tables_hints):\n",
    "    return SQL_QUERY_PROMPT_TEMPLATE.format(question=question, db_schema=db_schema, tables_hints=tables_hints)\n",
    "\n",
    "def generate_answer_generation_prompt(question, data):\n",
    "    return ANSWER_GENERATION_PROMPT_TEMPLATE.format(question=question, returned_schema=str(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0129143-e9f3-4608-a9d5-27d7f771dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sql_execution(sql_response, sql_schema):\n",
    "    results = tabulate(sql_response, tablefmt=\"jira\", headers=sql_schema)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0120465",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c883cb-5f94-4663-86f7-47fa79d5fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schemas(cursor, table_hints=None):\n",
    "    '''\n",
    "    get the schema information from this database\n",
    "    '''\n",
    "    tableQuery=\"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "    tableList=cursor.execute(tableQuery).fetchall()\n",
    "    schemas = []\n",
    "    tables = {}\n",
    "    for table in tableList:\n",
    "        tableName=table[0]\n",
    "        columnQuery=\"PRAGMA table_info('%s')\" % tableName\n",
    "        schema=cursor.execute(columnQuery).fetchall()\n",
    "        tables[tableName] = schema\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912fbe7-b6fb-439a-aef0-8ce7cb35b6a8",
   "metadata": {},
   "source": [
    "# Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9656c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_fun(database_name: str) -> sqlite3.Connection:\n",
    "    \"\"\"\n",
    "    Connect to an SQLite database and return a connection object.\n",
    "\n",
    "    Parameters:\n",
    "        database_name (str): The name (or path) of the SQLite database file to connect to.\n",
    "\n",
    "    Returns:\n",
    "        sqlite3.Connection or None: A connection object if the connection is successful,\n",
    "        or None if there is an error.\n",
    "\n",
    "    Example usage:\n",
    "        db_name = 'your_database_name.db'\n",
    "        connection = connect_fun(db_name)\n",
    "        \n",
    "        if connection:\n",
    "            print(f\"Connected to {db_name}\")\n",
    "            # You can now use 'connection' to interact with the database.\n",
    "        else:\n",
    "            print(\"Connection failed.\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection = sqlite3.connect(database_name)\n",
    "        return connection\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def query_fun(question: str, conn: sqlite3.Connection, tables_hints: List[str]=None, debug:bool=False) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer to a question based on an SQLite database and question context.\n",
    "\n",
    "    Parameters:\n",
    "        question (str): The user's question.\n",
    "        tables_hints (List[str]): List of table names to consider in the query.\n",
    "        conn (sqlite3.Connection): A connection to the SQLite database.\n",
    "\n",
    "    Returns:\n",
    "        str: The answer to the question.\n",
    "\n",
    "    Example usage:\n",
    "        question = \"How many customers are there in the database?\"\n",
    "        table_hints = [\"customers\"]\n",
    "        connection = sqlite3.connect(\"your_database.db\")\n",
    "        answer = query_fun(question, table_hints, connection)\n",
    "        print(answer)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Step 0: Get related tables based on all schemas and table hints\n",
    "        related_schemas = get_schemas(cursor, tables_hints) \n",
    "\n",
    "        if debug:\n",
    "            print(\"Related schemas: \", related_schemas) \n",
    "        \n",
    "        # Step 1: Generate an SQL query based on the question and table hints.\n",
    "        sql_query = generate_sql_query(sql_model, sql_tokenizer, question, related_schemas, tables_hints)\n",
    "\n",
    "        if debug:\n",
    "            print(\"SQL query: \", sql_query)\n",
    "\n",
    "        # Step 2: Execute the SQL query and fetch the results.\n",
    "        response = cursor.execute(sql_query)\n",
    "\n",
    "        # Step 3: Obtain records from response and schema information (column names) from the cursor description.\n",
    "        records = response.fetchall()\n",
    "        reponse_schema = [desc[0] for desc in cursor.description]\n",
    "        sql_response = format_sql_execution(records, reponse_schema)\n",
    "\n",
    "        if debug:\n",
    "            print(\"SQL execution response: \", sql_response)\n",
    "\n",
    "        # Step 4: Process the query result and generate an answer with context using LLM.\n",
    "        answer = generate_answer_with_context(answer_model, answer_tokenizer, question, sql_response)\n",
    "\n",
    "        return sql_query, answer\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"SQLite Error: {e}\")\n",
    "        return \"An error occurred while processing the query.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"An error occurred.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ef412",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3bae86-c7ae-4f3e-a263-b0384d89f703",
   "metadata": {},
   "source": [
    "![alt](/mnt/4TBSSD/pmkhoi/huawei-sql/huawei-arena-2023/imgs/chinook-er-diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe602ce-f0c8-4f94-b6bf-1507ef29d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = connect_fun('/home/mpham/workspace/huawei-arena-2023/data/chinook/Chinook_Sqlite.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2938e-3afc-4534-97cd-82985e517369",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()\n",
    "related_schemas = get_schemas(cursor, table_hints=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5760d2ed-92dd-4b00-be85-090e89ad3f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "related_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ff1f4-2ce2-45ed-918f-cab59a5797ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    # \"What is the highest sales of three salesman person? Give me the salesperson's name and his or her total sales\",\n",
    "    # \"In 1981 which team picked overall 148?\"\n",
    "\n",
    "    \"Find me 5 random song track names\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139cdf9c-99ab-4115-9866-3c8013059c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sql_command, text_response = query_fun(\n",
    "    question=questions[0],\n",
    "    conn=connection,\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47039c6-097b-4daa-949a-9a19c447d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4530a2-a403-4d32-a5b5-5aa23e8320ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68856c3a-30ca-452c-9b94-3f04bdb71695",
   "metadata": {},
   "source": [
    "# Testcases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5424254a-8e78-491b-8fd5-7d2a734e2ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_db_schema = \"\"\"\n",
    "CREATE TABLE products (\n",
    "  product_id INTEGER PRIMARY KEY, -- Unique ID for each product\n",
    "  name VARCHAR(50), -- Name of the product\n",
    "  price DECIMAL(10,2), -- Price of each unit of the product\n",
    "  quantity INTEGER  -- Current quantity in stock\n",
    ");\n",
    "\n",
    "CREATE TABLE customers (\n",
    "   customer_id INTEGER PRIMARY KEY, -- Unique ID for each customer\n",
    "   name VARCHAR(50), -- Name of the customer\n",
    "   address VARCHAR(100) -- Mailing address of the customer\n",
    ");\n",
    "\n",
    "CREATE TABLE salespeople (\n",
    "  salesperson_id INTEGER PRIMARY KEY, -- Unique ID for each salesperson\n",
    "  name VARCHAR(50), -- Name of the salesperson\n",
    "  region VARCHAR(50) -- Geographic sales region\n",
    ");\n",
    "\n",
    "CREATE TABLE sales (\n",
    "  sale_id INTEGER PRIMARY KEY, -- Unique ID for each sale\n",
    "  product_id INTEGER, -- ID of product sold\n",
    "  customer_id INTEGER,  -- ID of customer who made purchase\n",
    "  salesperson_id INTEGER, -- ID of salesperson who made the sale\n",
    "  sale_date DATE, -- Date the sale occurred\n",
    "  quantity INTEGER -- Quantity of product sold\n",
    ");\n",
    "\n",
    "CREATE TABLE product_suppliers (\n",
    "  supplier_id INTEGER PRIMARY KEY, -- Unique ID for each supplier\n",
    "  product_id INTEGER, -- Product ID supplied\n",
    "  supply_price DECIMAL(10,2) -- Unit price charged by supplier\n",
    ");\n",
    "\n",
    "-- sales.product_id can be joined with products.product_id\n",
    "-- sales.customer_id can be joined with customers.customer_id\n",
    "-- sales.salesperson_id can be joined with salespeople.salesperson_id\n",
    "-- product_suppliers.product_id can be joined with products.product_id\n",
    "\"\"\"\n",
    "\n",
    "question = \"What product has the biggest fall in sales in 2022 compared to 2021? \\\n",
    "            Give me the product name, the sales amount in both years, and the difference.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d37f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(generate_sql_query(sql_model, sql_tokenizer, question=question, db_schema=sales_db_schema, tables_hints=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc48f98-0039-4480-a4a9-95190fe88687",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_schema = \"\"\"\n",
    "Create Category table\n",
    "CREATE TABLE Category (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    title TEXT,\n",
    "    parent_id INTEGER,\n",
    "    FOREIGN KEY (parent_id) REFERENCES Category(id)\n",
    ");\n",
    "\n",
    "-- Create Post table\n",
    "CREATE TABLE Post (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    title TEXT,\n",
    "    content TEXT,\n",
    "    created_date DATETIME,\n",
    "    last_modified_date DATETIME,\n",
    "    created_id INTEGER,\n",
    "    last_modified_id INTEGER,\n",
    "    category_id INTEGER,\n",
    "    FOREIGN KEY (created_id) REFERENCES User(id),\n",
    "    FOREIGN KEY (last_modified_id) REFERENCES User(id),\n",
    "    FOREIGN KEY (category_id) REFERENCES Category(id)\n",
    ");\n",
    "\n",
    "-- Create Comment table\n",
    "CREATE TABLE Comment (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    post_id INTEGER,\n",
    "    content TEXT,\n",
    "    created_date DATETIME,\n",
    "    last_modified_date DATETIME,\n",
    "    created_id INTEGER,\n",
    "    parent_id INTEGER,\n",
    "    FOREIGN KEY (post_id) REFERENCES Post(id),\n",
    "    FOREIGN KEY (created_id) REFERENCES User(id),\n",
    "    FOREIGN KEY (parent_id) REFERENCES Comment(id)\n",
    ");\n",
    "\n",
    "-- Create User table\n",
    "CREATE TABLE User (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    username TEXT,\n",
    "    email TEXT,\n",
    "    first_name TEXT,\n",
    "    last_name TEXT\n",
    ");\n",
    "\n",
    "-- Create Reaction table\n",
    "CREATE TABLE Reaction (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    user_id INTEGER,\n",
    "    content_id INTEGER,\n",
    "    content_type TEXT,\n",
    "    reaction_type TEXT,\n",
    "    FOREIGN KEY (user_id) REFERENCES User(id),\n",
    "    FOREIGN KEY (content_id) REFERENCES Post(id) ON DELETE CASCADE,\n",
    "    FOREIGN KEY (content_id) REFERENCES Comment(id) ON DELETE CASCADE\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "sns_questions = [\n",
    "    'write SQL query to list all posts that belong to the category \"Vacancies\" \\\n",
    "    and its subordinates as long as the posts need to have at least 5 comments and 3 reactions.',\n",
    "\n",
    "    'write SQL query to list all posts that belong to the category \"Vacancies\" \\\n",
    "    and its subordinates as long as the posts need to have at least 5 comments \\\n",
    "    and at least 10 reactions across either the posts or their comments.',\n",
    "\n",
    "    'what are the full name of the top 5 users whose posts or comments having \\\n",
    "    the highest number of reactions of \"Like\" or \"Love\".',\n",
    "\n",
    "    \"Who is the most active users in commenting on across the posts that contain \\\n",
    "    the keyword 'education' and 'policy'  as well as the posts' comments from June 2022 to September 2023\"\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee89933-68cd-4686-9312-c0fee441aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(generate_sql_query(sql_model, sql_tokenizer, question=sns_questions[0], db_schema=sns_schema, tables_hints=None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
