{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91433d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d25df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6605a324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Sep 22 05:03:46 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   42C    P8    17W / 350W |  13329MiB / 24268MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1577      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "|    0   N/A  N/A   3604959      C   ...s/huawei_arena/bin/python    13321MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53db5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "import numpy as np\n",
    "from schema_item_filter import SchemaItemClassifierInference, filter_schema\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "import sys\n",
    "\n",
    "from utils.db_utils import check_sql_executability, get_db_schema_sequence, get_matched_content_sequence, detect_special_char\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3201fcce",
   "metadata": {},
   "source": [
    "# Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34342fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004854917526245117,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Downloading (…)okenizer_config.json",
       "rate": null,
       "total": 717,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6261437dcbfb42568a788786a86a85d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/717 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006425619125366211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Downloading (…)olve/main/vocab.json",
       "rate": null,
       "total": 776993,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44c56f7fcdc4e58baf7f02d885a2980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005912065505981445,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Downloading (…)olve/main/merges.txt",
       "rate": null,
       "total": 441810,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5921308dfe4c38a4b197584924dbc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00579524040222168,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Downloading (…)/main/tokenizer.json",
       "rate": null,
       "total": 2057423,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f477584f00304b18a6dd93b9778a3433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03851890563964844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Downloading (…)cial_tokens_map.json",
       "rate": null,
       "total": 564,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558cdd1eea814a318a013a5776717aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006148338317871094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Downloading (…)lve/main/config.json",
       "rate": null,
       "total": 1025,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fd23a4b3b44cd9afac813fb6ef5691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005950927734375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Downloading (…)model.bin.index.json",
       "rate": null,
       "total": 32678,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea01e35aaeb543aa835e013a990f7ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/32.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0035953521728515625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Downloading shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57063656970e4782b61f0048a31b3880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0068013668060302734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Downloading (…)l-00001-of-00002.bin",
       "rate": null,
       "total": 9998305841,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253bc0aec42b452a94934b3876930a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006223440170288086,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Downloading (…)l-00002-of-00002.bin",
       "rate": null,
       "total": 2175073723,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34acfb7061744c1a3a66790d7a3f4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/2.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0025653839111328125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cb957a7e384df78a1e23cbecf8f678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006569862365722656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Downloading (…)neration_config.json",
       "rate": null,
       "total": 111,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71cd758b5bef47a7ae9b6c488da1c8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPTBigCodeForCausalLM(\n",
       "  (transformer): GPTBigCodeModel(\n",
       "    (wte): Embedding(49152, 2816)\n",
       "    (wpe): Embedding(8192, 2816)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-35): 36 x GPTBigCodeBlock(\n",
       "        (ln_1): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTBigCodeAttention(\n",
       "          (c_attn): Linear(in_features=2816, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=2816, out_features=2816, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTBigCodeMLP(\n",
       "          (c_fc): Linear(in_features=2816, out_features=11264, bias=True)\n",
       "          (c_proj): Linear(in_features=11264, out_features=2816, bias=True)\n",
       "          (act): PytorchGELUTanh()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2816, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = 2048\n",
    "max_new_tokens = 256\n",
    "model_name = \"seeklhy/codes-3b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    # load_in_8bit=True,\n",
    "    # load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=True,\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22403a51",
   "metadata": {},
   "source": [
    "### Prepare SQL query prompt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43e24f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_QUERY_PROMPT_TEMPLATE = \"\"\"\n",
    "### Instructions:\n",
    "Your task is to convert a question into a SQL query, given a SQLlite database schema\n",
    "Adhere to these rules:\n",
    "- ****\n",
    "- **Deliberately go through the question and database schema word by word** to appropriately answer the question\n",
    "- **Use Table Aliases** to prevent ambiguity. For example, `SELECT table1.col1, table2.col1 FROM table1 JOIN table2 ON table1.id = table2.id`.\n",
    "- When creating a ratio, always cast the numerator as float.\n",
    "### Input:\n",
    "Generate a SQL query that answers the question `{question}`.\n",
    "This query will run on a database whose schema is represented in this string:\n",
    "{db_schema}\n",
    "\n",
    "### Response:\n",
    "Based on your instructions, here is the SQL query I have generated to answer the question `{question}`:\n",
    "```sql\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3991042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_query_generation_prompt(question, db_schema):\n",
    "    return SQL_QUERY_PROMPT_TEMPLATE.format(question=question, db_schema=db_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14950ec2",
   "metadata": {},
   "source": [
    "### SQL Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "085ea387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_query(question: str, db_schema: str, tables_hints: List[str], num_beams=5) -> str:\n",
    "    # Implement logic to generate an SQL query based on the question and table hints.\n",
    "    # Replace the \"pass\" with a calling function to LLM\n",
    "    \n",
    "    # Handle the case when table hints are empty or invalid.\n",
    "    if not tables_hints:\n",
    "        # Default behavior: Query all tables\n",
    "        pass\n",
    "    \n",
    "    # Handle the general case\n",
    "    # Example: \"SELECT COUNT(*) FROM customers\"\n",
    "    prompt = generate_sql_query_generation_prompt(question, db_schema)\n",
    "    eos_token_id = tokenizer.convert_tokens_to_ids([\"```\"])[0]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\") # or to(\"cpu\")\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=eos_token_id,\n",
    "        pad_token_id=eos_token_id,\n",
    "        max_new_tokens=400,\n",
    "        do_sample=False,\n",
    "        num_beams=num_beams\n",
    "    )\n",
    "    \n",
    "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    sql_query = outputs[0].split(\"```sql\")[-1].split(\"```\")[0].split(\";\")[0].strip() + \";\"\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return sql_query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a48903f",
   "metadata": {},
   "source": [
    "### Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8af4d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_db_schema = \"\"\"\n",
    "CREATE TABLE products (\n",
    "  product_id INTEGER PRIMARY KEY, -- Unique ID for each product\n",
    "  name VARCHAR(50), -- Name of the product\n",
    "  price DECIMAL(10,2), -- Price of each unit of the product\n",
    "  quantity INTEGER  -- Current quantity in stock\n",
    ");\n",
    "\n",
    "CREATE TABLE customers (\n",
    "   customer_id INTEGER PRIMARY KEY, -- Unique ID for each customer\n",
    "   name VARCHAR(50), -- Name of the customer\n",
    "   address VARCHAR(100) -- Mailing address of the customer\n",
    ");\n",
    "\n",
    "CREATE TABLE salespeople (\n",
    "  salesperson_id INTEGER PRIMARY KEY, -- Unique ID for each salesperson\n",
    "  name VARCHAR(50), -- Name of the salesperson\n",
    "  region VARCHAR(50) -- Geographic sales region\n",
    ");\n",
    "\n",
    "CREATE TABLE sales (\n",
    "  sale_id INTEGER PRIMARY KEY, -- Unique ID for each sale\n",
    "  product_id INTEGER, -- ID of product sold\n",
    "  customer_id INTEGER,  -- ID of customer who made purchase\n",
    "  salesperson_id INTEGER, -- ID of salesperson who made the sale\n",
    "  sale_date DATE, -- Date the sale occurred\n",
    "  quantity INTEGER -- Quantity of product sold\n",
    ");\n",
    "\n",
    "CREATE TABLE product_suppliers (\n",
    "  supplier_id INTEGER PRIMARY KEY, -- Unique ID for each supplier\n",
    "  product_id INTEGER, -- Product ID supplied\n",
    "  supply_price DECIMAL(10,2) -- Unit price charged by supplier\n",
    ");\n",
    "\n",
    "-- sales.product_id can be joined with products.product_id\n",
    "-- sales.customer_id can be joined with customers.customer_id\n",
    "-- sales.salesperson_id can be joined with salespeople.salesperson_id\n",
    "-- product_suppliers.product_id can be joined with products.product_id\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3633cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What product has the biggest fall in sales in 2022 compared to 2021? \\\n",
    "            Give me the product name, the sales amount in both years, and the difference.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5105d62c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT\n",
      "  products.name AS product_name,\n",
      "  sales_2021.sales_amount AS sales_amount_2021,\n",
      "  sales_2022.sales_amount AS sales_amount_2022,\n",
      "  sales_2022.sales_amount - sales_2021.sales_amount AS difference\n",
      "FROM products\n",
      "LEFT JOIN (\n",
      "  SELECT\n",
      "    product_id,\n",
      "    SUM(quantity * price) AS sales_amount\n",
      "  FROM sales\n",
      "  WHERE sale_date BETWEEN '2021-01-01' AND '2021-12-31'\n",
      "  GROUP BY product_id\n",
      ") AS sales_2021 ON products.product_id = sales_2021.product_id\n",
      "LEFT JOIN (\n",
      "  SELECT\n",
      "    product_id,\n",
      "    SUM(quantity * price) AS sales_amount\n",
      "  FROM sales\n",
      "  WHERE sale_date BETWEEN '2022-01-01' AND '2022-12-31'\n",
      "  GROUP BY product_id\n",
      ") AS sales_2022 ON products.product_id = sales_2022.product_id\n",
      "ORDER BY difference DESC\n",
      "LIMIT 1;\n",
      "CPU times: user 7.9 s, sys: 0 ns, total: 7.9 s\n",
      "Wall time: 7.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# num_beam = 5\n",
    "print(generate_sql_query(question=question, db_schema=sales_db_schema, tables_hints=None, num_beams=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99eff9",
   "metadata": {},
   "source": [
    "# Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62eca896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(sql, schema_items):\n",
    "    sql = sql.replace(\"\\n\", \" \")\n",
    "    for table in schema_items:\n",
    "        for column_name in table[\"column_names\"]:\n",
    "            special_char_in_column_name = detect_special_char(column_name)\n",
    "            if special_char_in_column_name and column_name in sql and \"`\"+column_name+\"`\" not in sql:\n",
    "                sql = sql.replace(column_name, \"`\"+column_name+\"`\")\n",
    "    sql = sql.replace(\" order \", \" `order` \")\n",
    "    return sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64480724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the skeleton of the input text\n",
    "def extract_skeleton(text):\n",
    "    tokens_and_tags = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "\n",
    "    output_tokens = []\n",
    "    for token, tag in tokens_and_tags:\n",
    "        if tag in ['NN', 'NNP', 'NNS', 'NNPS', 'CD', 'SYM', 'FW', 'IN']:\n",
    "            output_tokens.append(\"_\")\n",
    "        elif token in ['$', \"''\", '(', ')', ',', '--', '.', ':']:\n",
    "            pass\n",
    "        else:\n",
    "            output_tokens.append(token)\n",
    "    \n",
    "    text_skeleton = \" \".join(output_tokens)\n",
    "    text_skeleton = text_skeleton.replace(\"_ 's\", \"_\")\n",
    "    text_skeleton = text_skeleton.replace(\" 's\", \"'s\")\n",
    "\n",
    "    while(\"_ _\" in text_skeleton):\n",
    "        text_skeleton = text_skeleton.replace(\"_ _\", \"_\")\n",
    "    while(\"_ , _\" in text_skeleton):\n",
    "        text_skeleton = text_skeleton.replace(\"_ , _\", \"_\")\n",
    "    \n",
    "    if text_skeleton.startswith(\"_ \"):\n",
    "        text_skeleton = text_skeleton[2:]\n",
    "    \n",
    "    return text_skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5b1da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_ids_and_attention_mask(tokenizer, input_seq, max_input_length, device):\n",
    "    input_ids = tokenizer(input_seq , truncation = False)[\"input_ids\"]\n",
    "\n",
    "    if len(input_ids) <= max_input_length:\n",
    "        input_ids = input_ids\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "    else:\n",
    "        if tokenizer.name_or_path == \"THUDM/codegeex2-6b\":\n",
    "            input_ids = [64790, 64792] + input_ids[-(max_input_length-2):]\n",
    "        else:\n",
    "            input_ids = [tokenizer.bos_token_id] + input_ids[-(max_input_length-1):]\n",
    "\n",
    "        attention_mask = [1] * max_input_length\n",
    "    \n",
    "    print(\"len(input_ids):\", len(input_ids))\n",
    " \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor([input_ids]).to(device), # torch.int64\n",
    "        \"attention_mask\": torch.tensor([attention_mask]).to(device) # torch.int64\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f65a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cross_domain_input_seq(opt, eval_data, demonstration_set, similarity):\n",
    "    top_k_indices = sorted(range(len(similarity)), key = lambda x: similarity[x], reverse = True)[:opt.num_of_demonstrations]\n",
    "    # top_k_indices = list(reversed(top_k_indices))\n",
    "    # top_k_indices = random.sample(range(len(similarity)), opt.num_of_demonstrations)\n",
    "    print(top_k_indices)\n",
    "    print(similarity[top_k_indices])\n",
    "\n",
    "    input_seq = \"\"\n",
    "    for idx in top_k_indices:\n",
    "        demonstration_sql = demonstration_set[idx][\"sql\"]\n",
    "        if demonstration_sql.endswith(\";\"):\n",
    "            demonstration_sql = demonstration_sql[:-1].strip() + \" ;\"\n",
    "        else:\n",
    "            demonstration_sql = demonstration_sql.strip() + \" ;\"\n",
    "\n",
    "        input_seq += demonstration_set[idx][\"schema_sequence\"] + \"\\n\" + demonstration_set[idx][\"content_sequence\"] + \"\\n\" + \\\n",
    "            demonstration_set[idx][\"text\"] + \"\\n\" + demonstration_sql + \"\\n\\n\"\n",
    "\n",
    "    input_seq += eval_data[\"schema_sequence\"] + \"\\n\" + eval_data[\"content_sequence\"] + \"\\n\" + eval_data[\"text\"] + \"\\n\"\n",
    "    # print(input_seq)\n",
    "    # print(\"-\"*30)\n",
    "\n",
    "    return input_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "555c10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2sql_func(model, text2sql_input_seq, tokenizer, max_tokens, max_new_tokens):\n",
    "    inputs = prepare_input_ids_and_attention_mask(\n",
    "        tokenizer, \n",
    "        text2sql_input_seq, \n",
    "        max_tokens - max_new_tokens,\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generate_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens = max_new_tokens,\n",
    "            num_beams = 4,\n",
    "            num_return_sequences = 4,\n",
    "            use_cache = True\n",
    "        )\n",
    "\n",
    "    generated_sqls = tokenizer.batch_decode(generate_ids[:, input_length:], skip_special_tokens = True, clean_up_tokenization_spaces = False)\n",
    "\n",
    "    return generated_sqls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bf82c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
