{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee5cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers accelerate bitsandbytes scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95051031",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.33.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf91af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "716b7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tokenizers>=0.13.3 protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba01ef1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df13b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from typing import List\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c60f9",
   "metadata": {},
   "source": [
    "# Load the model\n",
    "\n",
    "Set parameter load_in_4bit=True for a model size of 12GB, otherwise the model will be too big. \n",
    "If you work on A100 machine, parameter torch_dtype=torch.bfloat16 can be set to True instead.\n",
    "\n",
    "Can read more here: https://huggingface.co/blog/4bit-transformers-bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8276f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824140942b0d4957af9d86c627ccd2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc84819f932f40dca7912d3e008a4836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6369f90e862d48449a59091d9aaf9d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d69dbce5c34d2884be8acfbb42abf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197094307d8b483ba96d049db408b55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee3ce36ac0b42fc97a2a9b213a3a67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a53649c66942cd952e284c7343fe40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token='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')\n",
    "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                         trust_remote_code=True,\n",
    "                                         load_in_4bit=True,\n",
    "                                         device_map=\"auto\",\n",
    "                                         use_cache=True,\n",
    "                                         token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de47a1f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.llama.tokenization_llama_fast because of the following error (look up to see its traceback):\ntokenizers>=0.13.3 is required for a normal functioning of this module, but found tokenizers==0.13.2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/transformers/utils/import_utils.py:1184\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama_fast.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversational\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conversation\n\u001b[0;32m---> 29\u001b[0m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers>=0.13.3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available():\n",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/transformers/utils/versions.py:111\u001b[0m, in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m op, want_ver \u001b[38;5;129;01min\u001b[39;00m wanted\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 111\u001b[0m     \u001b[43m_compare_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgot_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwant_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/transformers/utils/versions.py:44\u001b[0m, in \u001b[0;36m_compare_versions\u001b[0;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops[op](version\u001b[38;5;241m.\u001b[39mparse(got_ver), version\u001b[38;5;241m.\u001b[39mparse(want_ver)):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is required for a normal functioning of this module, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: tokenizers>=0.13.3 is required for a normal functioning of this module, but found tokenizers==0.13.2.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-chat-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhf_EngYQfDsJjMerNcktPzdUmBvRmtgDFYiGy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_EngYQfDsJjMerNcktPzdUmBvRmtgDFYiGy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m sequences \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTell me a story about a person writing a tutorial for installing llama 2 where every letter starts with s\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:728\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_fast \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_tokenizer_class\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFast\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    727\u001b[0m     tokenizer_class_candidate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_tokenizer_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mFast\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 728\u001b[0m     tokenizer_class \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer_class_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_class_candidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     tokenizer_class_candidate \u001b[38;5;241m=\u001b[39m config_tokenizer_class\n",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:418\u001b[0m, in \u001b[0;36mtokenizer_class_from_name\u001b[0;34m(class_name)\u001b[0m\n\u001b[1;32m    416\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/transformers/utils/import_utils.py:1174\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1174\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1175\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/transformers/utils/import_utils.py:1186\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1187\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1189\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.llama.tokenization_llama_fast because of the following error (look up to see its traceback):\ntokenizers>=0.13.3 is required for a normal functioning of this module, but found tokenizers==0.13.2."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, token='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')\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token='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'\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'Tell me a story about a person writing a tutorial for installing llama 2 where every letter starts with s\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc5d182",
   "metadata": {},
   "source": [
    "# Main functions required by the Huawei organisers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c65c8",
   "metadata": {},
   "source": [
    "### Build a Prompt template to get an SQL query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_QUERY_PROMPT_TEMPLATE = \"\"\"### Instructions:\n",
    "Your task is to convert a question into a SQL query, given a SQLlite database schema.\n",
    "Adhere to these rules:\n",
    "- **Deliberately go through the question and database schema word by word** to appropriately answer the question\n",
    "- **Use Table Aliases** to prevent ambiguity. For example, `SELECT table1.col1, table2.col1 FROM table1 JOIN table2 ON table1.id = table2.id`.\n",
    "- When creating a ratio, always cast the numerator as float\n",
    "### Input:\n",
    "Generate a SQL query that answers the question `{question}`.\n",
    "This query will run on a database whose schema is represented in this string:\n",
    "{db_schema}\n",
    "\n",
    "### Response:\n",
    "Based on your instructions, here is the SQL query I have generated to answer the question `{question}`:\n",
    "```sql\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2759502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_query_generation_prompt(question, db_schema):\n",
    "    return SQL_QUERY_PROMPT_TEMPLATE.format(question=question, db_schema=db_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1a5384",
   "metadata": {},
   "source": [
    "### Build a Prompt template to generate an answer based on the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_GENERATION_PROMPT_TEMPLATE = \"\"\"### Instructions:\n",
    "Your task is to convert a returned schema into an answer to the question:\n",
    "\n",
    "### Input:\n",
    "Generate an answer to the `{question}` based on the schema.\n",
    "This query will run on a database whose schema is represented in this string:\n",
    "{returned_schema}\n",
    "\n",
    "Your answer should be short, concise and straight to the point.\n",
    "\n",
    "### Response with the following format:\n",
    "Based on your question and the returned schema, here is the answer I have generated to answer the question `{question}`:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb45c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_generation_prompt(question, returned_schema):\n",
    "    return ANSWER_GENERATION_PROMPT_TEMPLATE.format(question=question, returned_schema=str(returned_schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b186d9c6",
   "metadata": {},
   "source": [
    "### Function to connect to database and return a connection object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfcd5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_fun(database_name: str) -> sqlite3.Connection:\n",
    "    \"\"\"\n",
    "    Connect to an SQLite database and return a connection object.\n",
    "\n",
    "    Parameters:\n",
    "        database_name (str): The name (or path) of the SQLite database file to connect to.\n",
    "\n",
    "    Returns:\n",
    "        sqlite3.Connection or None: A connection object if the connection is successful,\n",
    "        or None if there is an error.\n",
    "\n",
    "    Example usage:\n",
    "        db_name = 'your_database_name.db'\n",
    "        connection = connect_fun(db_name)\n",
    "        \n",
    "        if connection:\n",
    "            print(f\"Connected to {db_name}\")\n",
    "            # You can now use 'connection' to interact with the database.\n",
    "        else:\n",
    "            print(\"Connection failed.\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection = sqlite3.connect(database_name)\n",
    "        return connection\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a2946c",
   "metadata": {},
   "source": [
    "### Function to query a database and return an answer based on the context return from the SQL query\n",
    "\n",
    "Example: \n",
    "\n",
    "    Question: What is the months with the highest sales in the year? \n",
    "    Answer:   The highest sales happened in November, December and January \n",
    "    Prompt flow: \n",
    "        Question (str) -> **LLM SQL query model** -> SQL query -> returned schema -> **LLM to generate answer with context based on returned schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b049b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_query(question: str, db_schema: str, tables_hints: List[str]) -> str:\n",
    "    # Implement logic to generate an SQL query based on the question and table hints.\n",
    "    # Replace the \"pass\" with a calling function to LLM\n",
    "    \n",
    "    # Handle the case when table hints are empty or invalid.\n",
    "    if not tables_hints:\n",
    "        # Default behavior: Query all tables\n",
    "        pass\n",
    "    \n",
    "    # Handle the general case\n",
    "    # Example: \"SELECT COUNT(*) FROM customers\"\n",
    "    prompt = generate_sql_query_generation_prompt(question, db_schema)\n",
    "    eos_token_id = tokenizer.convert_tokens_to_ids([\"```\"])[0]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\") # or to(\"cpu\")\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=eos_token_id,\n",
    "        pad_token_id=eos_token_id,\n",
    "        max_new_tokens=400,\n",
    "        do_sample=False,\n",
    "        num_beams=5\n",
    "    )\n",
    "    \n",
    "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    sql_query = outputs[0].split(\"```sql\")[-1].split(\"```\")[0].split(\";\")[0].strip() + \";\"\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return sql_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e1a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_context(question: str, schema: List[str]) -> str:\n",
    "    answer_generation_prompt = generate_answer_generation_prompt(question, schema)\n",
    "    \n",
    "    ### Use the same procedure\n",
    "    eos_token_id = tokenizer.convert_tokens_to_ids([\"```\"])[0]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\") # or to(\"cpu\")\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=eos_token_id,\n",
    "        pad_token_id=eos_token_id,\n",
    "        max_new_tokens=400,\n",
    "        do_sample=False,\n",
    "        num_beams=5\n",
    "    )\n",
    "    \n",
    "    asnwer = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return answer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af224590",
   "metadata": {},
   "source": [
    "### Combine all functions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05315824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_fun(question: str, tables_hints: List[str], conn: sqlite3.Connection) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer to a question based on an SQLite database and question context.\n",
    "\n",
    "    Parameters:\n",
    "        question (str): The user's question.\n",
    "        tables_hints (List[str]): List of table names to consider in the query.\n",
    "        conn (sqlite3.Connection): A connection to the SQLite database.\n",
    "\n",
    "    Returns:\n",
    "        str: The answer to the question.\n",
    "\n",
    "    Example usage:\n",
    "        question = \"How many customers are there in the database?\"\n",
    "        table_hints = [\"customers\"]\n",
    "        connection = sqlite3.connect(\"your_database.db\")\n",
    "        answer = query_fun(question, table_hints, connection)\n",
    "        print(answer)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Generate an SQL query based on the question and table hints.\n",
    "        sql_query = generate_sql_query(question, tables_hints)\n",
    "\n",
    "        # Step 2: Execute the SQL query and fetch the results.\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(sql_query)\n",
    "\n",
    "        # Step 3: Obtain the schema information (column names) from the cursor description.\n",
    "        schema = [desc[0] for desc in cursor.description]\n",
    "\n",
    "        # Step 4: Process the query result and generate an answer with context using LLM.\n",
    "        answer = generate_answer_with_context(question, schema)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"SQLite Error: {e}\")\n",
    "        return \"An error occurred while processing the query.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"An error occurred.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6811e",
   "metadata": {},
   "source": [
    "# Now test the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d50d8d",
   "metadata": {},
   "source": [
    "### Test the function with sales db and question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425672c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_db_schema = \"\"\"\n",
    "CREATE TABLE products (\n",
    "  product_id INTEGER PRIMARY KEY, -- Unique ID for each product\n",
    "  name VARCHAR(50), -- Name of the product\n",
    "  price DECIMAL(10,2), -- Price of each unit of the product\n",
    "  quantity INTEGER  -- Current quantity in stock\n",
    ");\n",
    "\n",
    "CREATE TABLE customers (\n",
    "   customer_id INTEGER PRIMARY KEY, -- Unique ID for each customer\n",
    "   name VARCHAR(50), -- Name of the customer\n",
    "   address VARCHAR(100) -- Mailing address of the customer\n",
    ");\n",
    "\n",
    "CREATE TABLE salespeople (\n",
    "  salesperson_id INTEGER PRIMARY KEY, -- Unique ID for each salesperson\n",
    "  name VARCHAR(50), -- Name of the salesperson\n",
    "  region VARCHAR(50) -- Geographic sales region\n",
    ");\n",
    "\n",
    "CREATE TABLE sales (\n",
    "  sale_id INTEGER PRIMARY KEY, -- Unique ID for each sale\n",
    "  product_id INTEGER, -- ID of product sold\n",
    "  customer_id INTEGER,  -- ID of customer who made purchase\n",
    "  salesperson_id INTEGER, -- ID of salesperson who made the sale\n",
    "  sale_date DATE, -- Date the sale occurred\n",
    "  quantity INTEGER -- Quantity of product sold\n",
    ");\n",
    "\n",
    "CREATE TABLE product_suppliers (\n",
    "  supplier_id INTEGER PRIMARY KEY, -- Unique ID for each supplier\n",
    "  product_id INTEGER, -- Product ID supplied\n",
    "  supply_price DECIMAL(10,2) -- Unit price charged by supplier\n",
    ");\n",
    "\n",
    "-- sales.product_id can be joined with products.product_id\n",
    "-- sales.customer_id can be joined with customers.customer_id\n",
    "-- sales.salesperson_id can be joined with salespeople.salesperson_id\n",
    "-- product_suppliers.product_id can be joined with products.product_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0cef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What product has the biggest fall in sales in 2022 compared to 2021? \\\n",
    "            Give me the product name, the sales amount in both years, and the difference.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ddd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_sql_query(question=question, db_schema=sales_db_schema, tables_hints=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c29897",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the highest sales of three salesman person? Give me the salesperson's name and his or her total sales\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd1cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_sql_query(question=question, db_schema=sales_db_schema, tables_hints=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156b4cf1",
   "metadata": {},
   "source": [
    "### Get a dummy database\n",
    "\n",
    "![DB Image](./imgs/chinook-er-diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7617021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "# Define the SQLite database file name.\n",
    "db_file = \"./sample_db/Chinook_Sqlite.sqlite\"\n",
    "\n",
    "# Connect to the SQLite database.\n",
    "## creating a connection\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "## importing tables \n",
    "tables = pd.read_sql(\"\"\"SELECT *\n",
    "                        FROM sqlite_master\n",
    "                        WHERE type='table';\"\"\", conn)\n",
    "\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecfee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinook_db_schema =\"\"\"\n",
    "CREATE TABLE [Album]\n",
    "(\n",
    "    [AlbumId] INTEGER  NOT NULL,\n",
    "    [Title] NVARCHAR(160)  NOT NULL,\n",
    "    [ArtistId] INTEGER  NOT NULL,\n",
    "    CONSTRAINT [PK_Album] PRIMARY KEY  ([AlbumId]),\n",
    "    FOREIGN KEY ([ArtistId]) REFERENCES [Artist] ([ArtistId]) \n",
    "\t\tON DELETE NO ACTION ON UPDATE NO ACTION\n",
    ")\n",
    "CREATE TABLE [Artist]\n",
    "(\n",
    "    [ArtistId] INTEGER  NOT NULL,\n",
    "    [Name] NVARCHAR(120),\n",
    "    CONSTRAINT [PK_Artist] PRIMARY KEY  ([ArtistId])\n",
    ")\n",
    "CREATE TABLE [Customer]\n",
    "(\n",
    "    [CustomerId] INTEGER  NOT NULL,\n",
    "    [FirstName] NVARCHAR(40)  NOT NULL,\n",
    "    [LastName] NVARCHAR(20)  NOT NULL,\n",
    "    [Company] NVARCHAR(80),\n",
    "    [Address] NVARCHAR(70),\n",
    "    [City] NVARCHAR(40),\n",
    "    [State] NVARCHAR(40),\n",
    "    [Country] NVARCHAR(40),\n",
    "    [PostalCode] NVARCHAR(10),\n",
    "    [Phone] NVARCHAR(24),\n",
    "    [Fax] NVARCHAR(24),\n",
    "    [Email] NVARCHAR(60)  NOT NULL,\n",
    "    [SupportRepId] INTEGER,\n",
    "    CONSTRAINT [PK_Customer] PRIMARY KEY  ([CustomerId]),\n",
    "    FOREIGN KEY ([SupportRepId]) REFERENCES [Employee] ([EmployeeId]) \n",
    "\t\tON DELETE NO ACTION ON UPDATE NO ACTION\n",
    ")\n",
    "CREATE TABLE [Employee]\n",
    "(\n",
    "    [EmployeeId] INTEGER  NOT NULL,\n",
    "    [LastName] NVARCHAR(20)  NOT NULL,\n",
    "    [FirstName] NVARCHAR(20)  NOT NULL,\n",
    "    [Title] NVARCHAR(30),\n",
    "    [ReportsTo] INTEGER,\n",
    "    [BirthDate] DATETIME,\n",
    "    [HireDate] DATETIME,\n",
    "    [Address] NVARCHAR(70),\n",
    "    [City] NVARCHAR(40),\n",
    "    [State] NVARCHAR(40),\n",
    "    [Country] NVARCHAR(40),\n",
    "    [PostalCode] NVARCHAR(10),\n",
    "    [Phone] NVARCHAR(24),\n",
    "    [Fax] NVARCHAR(24),\n",
    "    [Email] NVARCHAR(60),\n",
    "    CONSTRAINT [PK_Employee] PRIMARY KEY  ([EmployeeId]),\n",
    "    FOREIGN KEY ([ReportsTo]) REFERENCES [Employee] ([EmployeeId]) \n",
    "\t\tON DELETE NO ACTION ON UPDATE NO ACTION\n",
    ")\n",
    "CREATE TABLE [Genre]\n",
    "(\n",
    "    [GenreId] INTEGER  NOT NULL,\n",
    "    [Name] NVARCHAR(120),\n",
    "    CONSTRAINT [PK_Genre] PRIMARY KEY  ([GenreId])\n",
    ")\n",
    "CREATE TABLE [Invoice]\n",
    "(\n",
    "    [InvoiceId] INTEGER  NOT NULL,\n",
    "    [CustomerId] INTEGER  NOT NULL,\n",
    "    [InvoiceDate] DATETIME  NOT NULL,\n",
    "    [BillingAddress] NVARCHAR(70),\n",
    "    [BillingCity] NVARCHAR(40),\n",
    "    [BillingState] NVARCHAR(40),\n",
    "    [BillingCountry] NVARCHAR(40),\n",
    "    [BillingPostalCode] NVARCHAR(10),\n",
    "    [Total] NUMERIC(10,2)  NOT NULL,\n",
    "    CONSTRAINT [PK_Invoice] PRIMARY KEY  ([InvoiceId]),\n",
    "    FOREIGN KEY ([CustomerId]) REFERENCES [Customer] ([CustomerId]) \n",
    "\t\tON DELETE NO ACTION ON UPDATE NO ACTION\n",
    ")\n",
    "CREATE TABLE [InvoiceLine]\n",
    "(\n",
    "    [InvoiceLineId] INTEGER  NOT NULL,\n",
    "    [InvoiceId] INTEGER  NOT NULL,\n",
    "    [TrackId] INTEGER  NOT NULL,\n",
    "    [UnitPrice] NUMERIC(10,2)  NOT NULL,\n",
    "    [Quantity] INTEGER  NOT NULL,\n",
    "    CONSTRAINT [PK_InvoiceLine] PRIMARY KEY  ([InvoiceLineId]),\n",
    "    FOREIGN KEY ([InvoiceId]) REFERENCES [Invoice] ([InvoiceId]) \n",
    "\t\tON DELETE NO ACTION ON UPDATE NO ACTION,\n",
    "    FOREIGN KEY ([TrackId]) REFERENCES [Track] ([TrackId]) \n",
    "\t\tON DELETE NO ACTION ON UPDATE NO ACTION\n",
    ")\n",
    "CREATE TABLE [MediaType]\n",
    "(\n",
    "    [MediaTypeId] INTEGER  NOT NULL,\n",
    "    [Name] NVARCHAR(120),\n",
    "    CONSTRAINT [PK_MediaType] PRIMARY KEY  ([MediaTypeId])\n",
    ")\n",
    "CREATE TABLE [Playlist]\n",
    "(\n",
    "    [PlaylistId] INTEGER  NOT NULL,\n",
    "    [Name] NVARCHAR(120),\n",
    "    CONSTRAINT [PK_Playlist] PRIMARY KEY  ([PlaylistId])\n",
    ")\n",
    "CREATE TABLE [PlaylistTrack]\n",
    "(\n",
    "    [PlaylistId] INTEGER  NOT NULL,\n",
    "    [TrackId] INTEGER  NOT NULL,\n",
    "    CONSTRAINT [PK_PlaylistTrack] PRIMARY KEY  ([PlaylistId], [TrackId]),\n",
    "    FOREIGN KEY ([PlaylistId]) REFERENCES [Playlist] ([PlaylistId]) \n",
    "\t\tON DELETE NO ACTION ON UPDATE NO ACTION,\n",
    "    FOREIGN KEY ([TrackId]) REFERENCES [Track] ([TrackId]) \n",
    "\t\tON DELETE NO ACTION ON UPDATE NO ACTION\n",
    ")\n",
    "CREATE TABLE [Track]\n",
    "(\n",
    "    [TrackId] INTEGER  NOT NULL,\n",
    "    [Name] NVARCHAR(200)  NOT NULL,\n",
    "    [AlbumId] INTEGER,\n",
    "    [MediaTypeId] INTEGER  NOT NULL,\n",
    "    [GenreId] INTEGER,\n",
    "    [Composer] NVARCHAR(220),\n",
    "    [Milliseconds] INTEGER  NOT NULL,\n",
    "    [Bytes] INTEGER,\n",
    "    [UnitPrice] NUMERIC(10,2)  NOT NULL,\n",
    "    CONSTRAINT [PK_Track] PRIMARY KEY  ([TrackId]),\n",
    "    FOREIGN KEY ([AlbumId]) REFERENCES [Album] ([AlbumId]) \n",
    "\t\tON DELETE NO ACTION ON UPDATE NO ACTION,\n",
    "    FOREIGN KEY ([GenreId]) REFERENCES [Genre] ([GenreId]) \n",
    "\t\tON DELETE NO ACTION ON UPDATE NO ACTION,\n",
    "    FOREIGN KEY ([MediaTypeId]) REFERENCES [MediaType] ([MediaTypeId]) \n",
    "\t\tON DELETE NO ACTION ON UPDATE NO ACTION\n",
    ")\n",
    "\"\"\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
