{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa74284f",
   "metadata": {},
   "source": [
    "# Notebook for fine tuning flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b17ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (0.3.10)\n",
      "Requirement already satisfied: transformers in /home/namtrinh/.local/lib/python3.9/site-packages (4.33.2)\n",
      "Requirement already satisfied: datasets in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (2.14.4)\n",
      "Requirement already satisfied: rouge-score in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: tensorboard in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (2.14.0)\n",
      "Requirement already satisfied: py7zr in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (0.20.6)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from pytesseract) (23.1)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /home/namtrinh/.local/lib/python3.9/site-packages (from pytesseract) (10.0.1)\n",
      "Requirement already satisfied: filelock in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/namtrinh/.local/lib/python3.9/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: xxhash in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from datasets) (2023.9.1)\n",
      "Requirement already satisfied: aiohttp in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: absl-py in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from rouge-score) (2.0.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from tensorboard) (1.58.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from tensorboard) (3.4.4)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from tensorboard) (4.24.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from tensorboard) (68.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from tensorboard) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from tensorboard) (2.3.7)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from tensorboard) (0.38.4)\n",
      "Requirement already satisfied: texttable in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from py7zr) (1.6.7)\n",
      "Requirement already satisfied: pycryptodomex>=3.6.6 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from py7zr) (3.19.0)\n",
      "Requirement already satisfied: pyzstd>=0.14.4 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from py7zr) (0.15.9)\n",
      "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from py7zr) (1.0.0)\n",
      "Requirement already satisfied: pybcj>=0.6.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from py7zr) (1.0.1)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from py7zr) (0.2.3)\n",
      "Requirement already satisfied: brotli>=1.0.9 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: inflate64>=0.3.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from py7zr) (0.3.1)\n",
      "Requirement already satisfied: psutil in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from py7zr) (5.9.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/namtrinh/.local/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard) (6.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract transformers datasets rouge-score nltk tensorboard py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a0125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa0c905",
   "metadata": {},
   "source": [
    "### Log in huggingface hub to push models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c249fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6437c4ae50634f129c981d85a02fc0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d04156",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c8d245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'head': ['age']}, {'body': ['height', 'weight']}]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def transform_sql_schema_to_list(sql_schema):\n",
    "    # Initialize an empty dictionary to store the table and column information\n",
    "    schema_dict = {}\n",
    "\n",
    "    # Split the SQL schema into individual CREATE TABLE statements\n",
    "    create_statements = sql_schema.split(\";\")\n",
    "\n",
    "    # Regular expression pattern to extract table and column names\n",
    "    pattern = r\"CREATE TABLE (\\w+) \\((.*?)\\)\"\n",
    "\n",
    "    # Iterate through each CREATE TABLE statement\n",
    "    for statement in create_statements:\n",
    "        match = re.match(pattern, statement.strip())\n",
    "        if match:\n",
    "            table_name = match.group(1)\n",
    "            column_definitions = match.group(2)\n",
    "            columns = [column.strip().split()[0] for column in column_definitions.split(\",\")]\n",
    "            schema_dict[table_name] = columns\n",
    "\n",
    "    # Convert the schema_dict into the desired format\n",
    "    result_list = [{table_name: columns} for table_name, columns in schema_dict.items()]\n",
    "\n",
    "    return result_list\n",
    "\n",
    "# Example usage:\n",
    "sql_schema = \"\"\"\n",
    "CREATE TABLE head (age INTEGER);\n",
    "CREATE TABLE body (height FLOAT, weight FLOAT);\n",
    "\"\"\"\n",
    "\n",
    "transformed_schema = transform_sql_schema_to_list(sql_schema)\n",
    "print(transformed_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b52a8",
   "metadata": {},
   "source": [
    "# Inference with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d2e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"juierror/flan-t5-text2sql-with-schema-v2\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"juierror/flan-t5-text2sql-with-schema-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ac846b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/namtrinh/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT count(*) FROM people_age AS T1 JOIN people_name AS T2 ON T1.people_id = T2.people_id WHERE T2.name = 'jui' AND T1.age < 25\n",
      "SELECT id FROM people_name WHERE name = 'jui' AND age < 25\n"
     ]
    }
   ],
   "source": [
    "def get_prompt(tables, question):\n",
    "    prompt = f\"\"\"convert question and table into SQL query. tables: {tables}. question: {question}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def prepare_input(question: str, tables: Dict[str, List[str]]):\n",
    "    tables = [f\"\"\"{table_name}({\",\".join(tables[table_name])})\"\"\" for table_name in tables]\n",
    "    tables = \", \".join(tables)\n",
    "    prompt = get_prompt(tables, question)\n",
    "    input_ids = tokenizer(prompt, max_length=512, return_tensors=\"pt\").input_ids\n",
    "    return input_ids\n",
    "\n",
    "def inference(question: str, tables: Dict[str, List[str]]) -> str:\n",
    "    input_data = prepare_input(question=question, tables=tables)\n",
    "    input_data = input_data.to(model.device)\n",
    "    outputs = model.generate(inputs=input_data, num_beams=10, top_k=10, max_length=512)\n",
    "    result = tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "print(inference(\"how many people with name jui and age less than 25\", {\n",
    "    \"people_name\": [\"id\", \"name\"],\n",
    "    \"people_age\": [\"people_id\", \"age\"]\n",
    "}))\n",
    "\n",
    "print(inference(\"what is id with name jui and age less than 25\", {\n",
    "    \"people_name\": [\"id\", \"name\", \"age\"]\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71609d50",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efadeb1",
   "metadata": {},
   "source": [
    "Text2SQL is a text2text-generation task. This means our model will take a text as input and generate an SQL as output. For this we want to understand how long our input and output will be to be able to efficiently batch our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "018fc965",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"b-mc2/sql-create-context\"\n",
    "train_ds = load_dataset(dataset_id, split='train[:80%]')\n",
    "valid_ds = load_dataset(dataset_id, split='train[80%:]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "663a6fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62862, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b18afc3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15715, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2695197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many heads of the departments are older than 56 ?',\n",
       " 'answer': 'SELECT COUNT(*) FROM head WHERE age > 56',\n",
       " 'context': 'CREATE TABLE head (age INTEGER)'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "174100cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    \"\"\"\n",
    "    Extracts maximum token length from the model configuration\n",
    "\n",
    "    :param model: Hugging Face model\n",
    "    \"\"\"\n",
    "\n",
    "    # Pull model configuration\n",
    "    conf = model.config\n",
    "    # Initialize a \"max_length\" variable to store maximum sequence length as null\n",
    "    max_length = None\n",
    "    # Find maximum sequence length in the model configuration and save it in \"max_length\" if found\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    # Set \"max_length\" to 1024 (default value) if maximum sequence length is not found in the model configuration\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7779dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset batch\n",
    "\n",
    "    :param batch: Dataset batch\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param max_length: Maximum number of tokens to emit from the tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length = max_length,\n",
    "        truncation = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77483ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 265\n",
      "Max target length: 261\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets(\n",
    "    [train_ds, valid_ds]).map(lambda x: \n",
    "                                              tokenizer(x[\"question\"]+x[\"context\"], truncation=True), \n",
    "                                              batched=True, \n",
    "                                              remove_columns=['question', 'answer', 'context' ])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets(\n",
    "    [train_ds, valid_ds]).map(lambda x: \n",
    "                                              tokenizer(x[\"answer\"], truncation=True), \n",
    "                                              batched=True, \n",
    "                                              remove_columns=['question', 'answer', 'context'])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3eea256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many heads of the departments are older than 56 ?',\n",
       " 'answer': 'SELECT COUNT(*) FROM head WHERE age > 56',\n",
       " 'context': 'CREATE TABLE head (age INTEGER)'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97221649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e659d9f57f468aa389f31857c12a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n",
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [f\"\"\"convert question and table into SQL query. tables: \\\n",
    "             {sample['context'][i]}. question: {sample['question'][i]}\"\"\" \n",
    "              for i, _ in enumerate(sample['context'])]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # print(model_inputs)\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"answer\"], \n",
    "                       max_length=max_target_length, \n",
    "                       padding=padding, \n",
    "                       truncation=True,\n",
    "                      )\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_tokenized_dataset = train_ds.map(preprocess_function, batched=True, remove_columns=['question', 'answer', 'context'])\n",
    "valid_tokenized_dataset = valid_ds.map(preprocess_function, batched=True, remove_columns=['question', 'answer', 'context'])\n",
    "\n",
    "# tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['question', 'answer', 'context'])\n",
    "print(f\"Keys of tokenized dataset: {list(train_tokenized_dataset.features)}\")\n",
    "print(f\"Keys of tokenized dataset: {list(valid_tokenized_dataset.features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcec4f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15715, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_tokenized_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d687332",
   "metadata": {},
   "source": [
    "# Evaluate and fine tune t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad2b2b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32101, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32101, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32101, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32101, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model_id = \"juierror/flan-t5-text2sql-with-schema-v2\"\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2b4cbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/namtrinh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8eb8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5601d133",
   "metadata": {},
   "source": [
    "### Start the finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa2add1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "# Hugging Face repository id\n",
    "repository_id = f\"{model_id.split('/')[1]}-{dataset_id}\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # Overflows with fp16\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"overall_f1\",\n",
    "    # push to hub parameters\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repository_id,\n",
    "    hub_token=HfFolder.get_token(),\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=valid_tokenized_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91251c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='19645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   17/19645 00:07 < 2:53:13, 1.89 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aff1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
