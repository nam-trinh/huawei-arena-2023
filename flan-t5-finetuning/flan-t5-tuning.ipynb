{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce4693a",
   "metadata": {},
   "source": [
    "# Notebook for fine tuning flan-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0bfd4687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: transformers in /home/namtrinh/.local/lib/python3.9/site-packages (4.33.2)\n",
      "Requirement already satisfied: datasets in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (2.14.4)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nltk in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (3.7)\n",
      "Collecting tensorboard\n",
      "  Obtaining dependency information for tensorboard from https://files.pythonhosted.org/packages/bc/a2/ff5f4c299eb37c95299a76015da3f30211468e29d8d6f1d011683279baee/tensorboard-2.14.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting py7zr\n",
      "  Obtaining dependency information for py7zr from https://files.pythonhosted.org/packages/2c/da/155bb1f692c067b9213c9c7b8c19a012a65027399606d623a25dfb1d3af1/py7zr-0.20.6-py3-none-any.whl.metadata\n",
      "  Downloading py7zr-0.20.6-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from pytesseract) (23.1)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /home/namtrinh/.local/lib/python3.9/site-packages (from pytesseract) (10.0.1)\n",
      "Requirement already satisfied: filelock in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/namtrinh/.local/lib/python3.9/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: xxhash in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from datasets) (2023.9.1)\n",
      "Requirement already satisfied: aiohttp in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from datasets) (3.8.5)\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Obtaining dependency information for absl-py from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from nltk) (1.3.2)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Obtaining dependency information for grpcio>=1.48.2 from https://files.pythonhosted.org/packages/5a/e8/c6bbb12ec726c80c05baaab5dc0d026798ef6e9f53edd7a3798655b9c242/grpcio-1.58.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading grpcio-1.58.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9d/44/5a992cb9d7bf8aaae73bc5adaf721ad08731c9d00c1c17999a8691404b0c/google_auth-2.23.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.23.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from tensorboard) (4.24.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from tensorboard) (68.0.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/02/52/fb9e51fba47951aabd7a6b25e41d73eae94208ccf62d886168096941a781/tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/9b/59/a7c32e3d8d0e546a206e0552a2c04444544f15c1da4a01df8938d20c6ffc/werkzeug-2.3.7-py3-none-any.whl.metadata\n",
      "  Downloading werkzeug-2.3.7-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from tensorboard) (0.38.4)\n",
      "Collecting texttable (from py7zr)\n",
      "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
      "Collecting pycryptodomex>=3.6.6 (from py7zr)\n",
      "  Obtaining dependency information for pycryptodomex>=3.6.6 from https://files.pythonhosted.org/packages/23/23/3f3d042c96ff7bece5b126365593b1f9c8e3ae62ce80d44e9da39c5e8a73/pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting pyzstd>=0.14.4 (from py7zr)\n",
      "  Obtaining dependency information for pyzstd>=0.14.4 from https://files.pythonhosted.org/packages/f7/1d/4cc7e9a7d934ed82ab9a2e4c0640c79d523cdf0440aae624947f8b3ef04a/pyzstd-0.15.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pyzstd-0.15.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting pyppmd<1.1.0,>=0.18.1 (from py7zr)\n",
      "  Downloading pyppmd-1.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting pybcj>=0.6.0 (from py7zr)\n",
      "  Downloading pybcj-1.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr)\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Collecting brotli>=1.0.9 (from py7zr)\n",
      "  Obtaining dependency information for brotli>=1.0.9 from https://files.pythonhosted.org/packages/e2/e6/4a730f6e5b5d538e92d09bc51bf69119914f29a222f9e1d65ae4abb27a4e/Brotli-1.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata\n",
      "  Downloading Brotli-1.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting inflate64>=0.3.1 (from py7zr)\n",
      "  Downloading inflate64-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (92 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from py7zr) (5.9.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/namtrinh/.local/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.16)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard) (6.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/namtrinh/anaconda3/envs/huawei_arena/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.11.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading py7zr-0.20.6-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Brotli-1.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.23.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.4/181.4 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.58.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyzstd-0.15.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "Downloading werkzeug-2.3.7-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=00f4195b0ec97341e0725e325b0ac96a71cf79805c36b89e61f1325418028ce3\n",
      "  Stored in directory: /home/namtrinh/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: texttable, brotli, werkzeug, tensorboard-data-server, pyzstd, pytesseract, pyppmd, pycryptodomex, pybcj, pyasn1, oauthlib, multivolumefile, inflate64, grpcio, cachetools, absl-py, rsa, rouge-score, requests-oauthlib, pyasn1-modules, py7zr, markdown, google-auth, google-auth-oauthlib, tensorboard\n",
      "Successfully installed absl-py-2.0.0 brotli-1.1.0 cachetools-5.3.1 google-auth-2.23.0 google-auth-oauthlib-1.0.0 grpcio-1.58.0 inflate64-0.3.1 markdown-3.4.4 multivolumefile-0.2.3 oauthlib-3.2.2 py7zr-0.20.6 pyasn1-0.5.0 pyasn1-modules-0.3.0 pybcj-1.0.1 pycryptodomex-3.19.0 pyppmd-1.0.0 pytesseract-0.3.10 pyzstd-0.15.9 requests-oauthlib-1.3.1 rouge-score-0.1.2 rsa-4.9 tensorboard-2.14.0 tensorboard-data-server-0.7.1 texttable-1.6.7 werkzeug-2.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract transformers datasets rouge-score nltk tensorboard py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "644cbe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f5c24c",
   "metadata": {},
   "source": [
    "### Log in huggingface hub to push models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d290467a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5998b14cd1a24f6baeccdcb3db8190b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166cca0b",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b39467e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'head': ['age']}, {'body': ['height', 'weight']}]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def transform_sql_schema_to_list(sql_schema):\n",
    "    # Initialize an empty dictionary to store the table and column information\n",
    "    schema_dict = {}\n",
    "\n",
    "    # Split the SQL schema into individual CREATE TABLE statements\n",
    "    create_statements = sql_schema.split(\";\")\n",
    "\n",
    "    # Regular expression pattern to extract table and column names\n",
    "    pattern = r\"CREATE TABLE (\\w+) \\((.*?)\\)\"\n",
    "\n",
    "    # Iterate through each CREATE TABLE statement\n",
    "    for statement in create_statements:\n",
    "        match = re.match(pattern, statement.strip())\n",
    "        if match:\n",
    "            table_name = match.group(1)\n",
    "            column_definitions = match.group(2)\n",
    "            columns = [column.strip().split()[0] for column in column_definitions.split(\",\")]\n",
    "            schema_dict[table_name] = columns\n",
    "\n",
    "    # Convert the schema_dict into the desired format\n",
    "    result_list = [{table_name: columns} for table_name, columns in schema_dict.items()]\n",
    "\n",
    "    return result_list\n",
    "\n",
    "# Example usage:\n",
    "sql_schema = \"\"\"\n",
    "CREATE TABLE head (age INTEGER);\n",
    "CREATE TABLE body (height FLOAT, weight FLOAT);\n",
    "\"\"\"\n",
    "\n",
    "transformed_schema = transform_sql_schema_to_list(sql_schema)\n",
    "print(transformed_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45ff81",
   "metadata": {},
   "source": [
    "# Inference with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "066f1f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"juierror/flan-t5-text2sql-with-schema-v2\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"juierror/flan-t5-text2sql-with-schema-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b39455f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/namtrinh/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT count(*) FROM people_age AS T1 JOIN people_name AS T2 ON T1.people_id = T2.people_id WHERE T2.name = 'jui' AND T1.age < 25\n",
      "SELECT id FROM people_name WHERE name = 'jui' AND age < 25\n"
     ]
    }
   ],
   "source": [
    "def get_prompt(tables, question):\n",
    "    prompt = f\"\"\"convert question and table into SQL query. tables: {tables}. question: {question}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def prepare_input(question: str, tables: Dict[str, List[str]]):\n",
    "    tables = [f\"\"\"{table_name}({\",\".join(tables[table_name])})\"\"\" for table_name in tables]\n",
    "    tables = \", \".join(tables)\n",
    "    prompt = get_prompt(tables, question)\n",
    "    input_ids = tokenizer(prompt, max_length=512, return_tensors=\"pt\").input_ids\n",
    "    return input_ids\n",
    "\n",
    "def inference(question: str, tables: Dict[str, List[str]]) -> str:\n",
    "    input_data = prepare_input(question=question, tables=tables)\n",
    "    input_data = input_data.to(model.device)\n",
    "    outputs = model.generate(inputs=input_data, num_beams=10, top_k=10, max_length=512)\n",
    "    result = tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "print(inference(\"how many people with name jui and age less than 25\", {\n",
    "    \"people_name\": [\"id\", \"name\"],\n",
    "    \"people_age\": [\"people_id\", \"age\"]\n",
    "}))\n",
    "\n",
    "print(inference(\"what is id with name jui and age less than 25\", {\n",
    "    \"people_name\": [\"id\", \"name\", \"age\"]\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211cda76",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c365a",
   "metadata": {},
   "source": [
    "Text2SQL is a text2text-generation task. This means our model will take a text as input and generate an SQL as output. For this we want to understand how long our input and output will be to be able to efficiently batch our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "be6cfa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"b-mc2/sql-create-context\"\n",
    "train_ds = load_dataset(dataset_id, split='train[:80%]')\n",
    "valid_ds = load_dataset(dataset_id, split='train[80%:]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d040d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "22c46ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62862, 3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8da65a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15715, 3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "977039e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many heads of the departments are older than 56 ?',\n",
       " 'answer': 'SELECT COUNT(*) FROM head WHERE age > 56',\n",
       " 'context': 'CREATE TABLE head (age INTEGER)'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1dc16b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt template for a prompt in the instruction dataset\n",
    "\n",
    "    :param sample: Prompt or sample from the instruction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize static strings for the prompt template\n",
    "    tables = transform_sql_schema_to_list(sample['context'])\n",
    "    SQL_QUERY_PROMPT_TEMPLATE = \"\"\"\n",
    "    convert question and table into SQL query. tables: {tables}. question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine a prompt with the static strings\n",
    "    sample[\"text\"] = SQL_QUERY_PROMPT_TEMPLATE.format(question=sample['question'], \n",
    "                                                      tables=tables,\n",
    "                                                     )\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a47816b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    \"\"\"\n",
    "    Extracts maximum token length from the model configuration\n",
    "\n",
    "    :param model: Hugging Face model\n",
    "    \"\"\"\n",
    "\n",
    "    # Pull model configuration\n",
    "    conf = model.config\n",
    "    # Initialize a \"max_length\" variable to store maximum sequence length as null\n",
    "    max_length = None\n",
    "    # Find maximum sequence length in the model configuration and save it in \"max_length\" if found\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    # Set \"max_length\" to 1024 (default value) if maximum sequence length is not found in the model configuration\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4a55397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset batch\n",
    "\n",
    "    :param batch: Dataset batch\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param max_length: Maximum number of tokens to emit from the tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length = max_length,\n",
    "        truncation = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7b112426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71f018b2fee461899975731b0bcfa6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/78577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 265\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35021f4c2eb64cce9added4f9ec6e180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/78577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 261\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets(\n",
    "    [train_ds, valid_ds]).map(lambda x: \n",
    "                                              tokenizer(x[\"question\"]+x[\"context\"], truncation=True), \n",
    "                                              batched=True, \n",
    "                                              remove_columns=['question', 'answer', 'context' ])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets(\n",
    "    [train_ds, valid_ds]).map(lambda x: \n",
    "                                              tokenizer(x[\"answer\"], truncation=True), \n",
    "                                              batched=True, \n",
    "                                              remove_columns=['question', 'answer', 'context'])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e1f04ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many heads of the departments are older than 56 ?',\n",
       " 'answer': 'SELECT COUNT(*) FROM head WHERE age > 56',\n",
       " 'context': 'CREATE TABLE head (age INTEGER)'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9b9b13b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n",
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [f\"\"\"convert question and table into SQL query. tables: \\\n",
    "             {sample['context'][i]}. question: {sample['question'][i]}\"\"\" \n",
    "              for i, _ in enumerate(sample['context'])]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # print(model_inputs)\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"answer\"], \n",
    "                       max_length=max_target_length, \n",
    "                       padding=padding, \n",
    "                       truncation=True,\n",
    "                      )\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_tokenized_dataset = train_ds.map(preprocess_function, batched=True, remove_columns=['question', 'answer', 'context'])\n",
    "valid_tokenized_dataset = valid_ds.map(preprocess_function, batched=True, remove_columns=['question', 'answer', 'context'])\n",
    "\n",
    "# tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['question', 'answer', 'context'])\n",
    "print(f\"Keys of tokenized dataset: {list(train_tokenized_dataset.features)}\")\n",
    "print(f\"Keys of tokenized dataset: {list(valid_tokenized_dataset.features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c149ceb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15715, 3)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_tokenized_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63798478",
   "metadata": {},
   "source": [
    "# Evaluate and fine tune t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c0775753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32101, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32101, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32101, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32101, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model_id = \"juierror/flan-t5-text2sql-with-schema-v2\"\n",
    "sql_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "sql_model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "923a53b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/namtrinh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "44bca0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44a41c",
   "metadata": {},
   "source": [
    "### Start the finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "be40421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "# Hugging Face repository id\n",
    "repository_id = f\"{model_id.split('/')[1]}-{dataset_id}\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # Overflows with fp16\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"overall_f1\",\n",
    "    # push to hub parameters\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repository_id,\n",
    "    hub_token=HfFolder.get_token(),\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=sql_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=valid_tokenized_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7f8e930c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='592' max='39290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  592/39290 02:57 < 3:14:29, 3.32 it/s, Epoch 0.08/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:1835\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1835\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1838\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1840\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1841\u001b[0m ):\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2690\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2688\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2690\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/accelerator.py:1985\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1984\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1985\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/huawei_arena/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f484bee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
